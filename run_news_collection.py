"""
Script principal de collecte de news avec la nouvelle configuration
Collecte sur une p√©riode plus longue avec tous les param√®tres de production

USAGE:
    source venv/bin/activate
    python run_news_collection.py
"""
import sys
from pathlib import Path
from datetime import datetime, timedelta

# V√©rifier environnement virtuel
if not hasattr(sys, 'real_prefix') and not (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
    print("\n‚ö†Ô∏è  ERREUR: Environnement virtuel non activ√©!")
    print("\nüí° Veuillez d'abord activer l'environnement virtuel:")
    print("   source venv/bin/activate")
    print("\nPuis relancez:")
    print("   python run_news_collection.py\n")
    sys.exit(1)

sys.path.insert(0, str(Path(__file__).parent))

from src.utils.logger import setup_logger
from src.collectors.hybrid_news_collector import HybridNewsCollector
from loguru import logger
import pandas as pd


def main():
    """Collecte principale de news avec la nouvelle configuration"""

    setup_logger(level="INFO")

    logger.info("=" * 80)
    logger.info("üöÄ COLLECTE PRINCIPALE DE NEWS - CONFIGURATION √âLARGIE")
    logger.info("=" * 80)

    # Param√®tres de collecte
    # Vous pouvez modifier ces valeurs selon vos besoins
    END_DATE = datetime.now().strftime("%Y-%m-%d")
    START_DATE = (datetime.now() - timedelta(days=15)).strftime("%Y-%m-%d")  # 15 derniers jours
    MIN_RELEVANCE_SCORE = 3.0  # Nouveau seuil
    MAX_RECORDS_PER_QUERY = 250  # Maximum de records par requ√™te GDELT
    DELAY_BETWEEN_REQUESTS = 2.0  # D√©lai entre les requ√™tes (secondes)
    TOP_N_GUARANTEED = 100  # Garantir les 100 meilleures news (0 = d√©sactiv√©)

    logger.info(f"\nüìÖ P√âRIODE DE COLLECTE:")
    logger.info(f"  ‚Ä¢ Du: {START_DATE}")
    logger.info(f"  ‚Ä¢ Au: {END_DATE}")
    logger.info(f"\n‚öôÔ∏è  PARAM√àTRES:")
    logger.info(f"  ‚Ä¢ Seuil de pertinence minimum: {MIN_RELEVANCE_SCORE}")
    logger.info(f"  ‚Ä¢ Top N garanti: {TOP_N_GUARANTEED} {'(activ√©)' if TOP_N_GUARANTEED > 0 else '(d√©sactiv√©)'}")
    logger.info(f"  ‚Ä¢ Records max par requ√™te: {MAX_RECORDS_PER_QUERY}")
    logger.info(f"  ‚Ä¢ D√©lai entre requ√™tes: {DELAY_BETWEEN_REQUESTS}s")

    # Initialiser le collecteur
    logger.info("\nüîß Initialisation du collecteur...")
    collector = HybridNewsCollector()

    # Lancer la collecte
    logger.info("\n" + "=" * 80)
    logger.info("‚ö° D√âMARRAGE DE LA COLLECTE")
    logger.info("=" * 80)
    logger.info("‚è≥ Cela peut prendre plusieurs minutes selon la p√©riode...")

    try:
        mapped_news = collector.collect_and_map(
            start_date=START_DATE,
            end_date=END_DATE,
            min_relevance_score=MIN_RELEVANCE_SCORE,
            max_records_per_query=MAX_RECORDS_PER_QUERY,
            delay=DELAY_BETWEEN_REQUESTS,
            top_n_guaranteed=TOP_N_GUARANTEED
        )

        if not mapped_news.empty:
            logger.info("\n" + "=" * 80)
            logger.info("‚úÖ COLLECTE TERMIN√âE AVEC SUCC√àS")
            logger.info("=" * 80)

            # Statistiques d√©taill√©es
            total_news = mapped_news['url'].nunique()
            total_associations = len(mapped_news)
            total_assets = mapped_news['asset'].nunique()

            logger.info(f"\nüìä STATISTIQUES GLOBALES:")
            logger.info(f"  ‚Ä¢ News uniques collect√©es: {total_news}")
            logger.info(f"  ‚Ä¢ Associations news-actifs: {total_associations}")
            logger.info(f"  ‚Ä¢ Actifs impact√©s: {total_assets}")

            # Distribution par cat√©gorie
            logger.info(f"\nüìÇ DISTRIBUTION PAR CAT√âGORIE:")
            category_counts = mapped_news['event_category'].value_counts()
            for category, count in category_counts.items():
                pct = (count / total_associations) * 100
                logger.info(f"  ‚Ä¢ {category:15s}: {count:4d} ({pct:5.1f}%)")

            # Distribution des scores
            logger.info(f"\nüìä DISTRIBUTION DES SCORES DE PERTINENCE:")
            score_ranges = [
                (3, 5, "3.0-5.0 (captur√© gr√¢ce au nouveau seuil)"),
                (5, 8, "5.0-8.0 (pertinence moyenne)"),
                (8, 10, "8.0-10.0 (pertinence √©lev√©e)"),
                (10, 999, "> 10.0 (tr√®s pertinent)")
            ]

            for min_s, max_s, label in score_ranges:
                count = len(mapped_news[(mapped_news['relevance_score'] >= min_s) &
                                        (mapped_news['relevance_score'] < max_s)])
                if count > 0:
                    pct = (count / total_associations) * 100
                    logger.info(f"  ‚Ä¢ {label:45s} | {count:4d} ({pct:5.1f}%)")

            # Top actifs impact√©s
            logger.info(f"\nüèÜ TOP 10 ACTIFS LES PLUS IMPACT√âS:")
            top_assets = mapped_news['asset'].value_counts().head(10)
            for rank, (asset, count) in enumerate(top_assets.items(), 1):
                avg_score = mapped_news[mapped_news['asset'] == asset]['relevance_score'].mean()
                logger.info(f"  {rank:2d}. {asset:20s} | {count:3d} news | Score moyen: {avg_score:.1f}")

            # Top √©v√©nements d√©tect√©s
            logger.info(f"\nüéØ TOP 10 √âV√âNEMENTS D√âTECT√âS:")
            event_counts = {}
            for events_str in mapped_news['matched_events'].dropna():
                for event in str(events_str).split(','):
                    event = event.strip()
                    if event:
                        event_counts[event] = event_counts.get(event, 0) + 1

            for rank, (event, count) in enumerate(sorted(event_counts.items(),
                                                          key=lambda x: x[1],
                                                          reverse=True)[:10], 1):
                logger.info(f"  {rank:2d}. {event:30s} | {count:3d} d√©tections")

            # Distribution par langue
            logger.info(f"\nüåç DISTRIBUTION PAR LANGUE:")
            lang_counts = mapped_news['language'].value_counts()
            for lang, count in lang_counts.items():
                pct = (count / total_associations) * 100
                logger.info(f"  ‚Ä¢ {lang}: {count:4d} ({pct:5.1f}%)")

            # Exemples de news r√©centes
            logger.info(f"\nüì∞ EXEMPLES DE NEWS R√âCENTES (top 5 par score):")
            top_news = mapped_news.nlargest(5, 'relevance_score')
            for idx, row in top_news.iterrows():
                logger.info(f"\n  [{row['relevance_score']:.1f}] {row['title'][:70]}...")
                logger.info(f"      Actif: {row['asset']} | √âv√©nement: {row['event_type']} | Source: {row['source']}")

            # Fichiers de sortie
            logger.info(f"\n" + "=" * 80)
            logger.info("üíæ FICHIERS G√âN√âR√âS")
            logger.info("=" * 80)

            raw_file = Path("data/raw/news/hybrid_news_raw.csv")
            mapped_file = Path("data/raw/news/hybrid_news_mapped.csv")

            logger.info(f"  ‚Ä¢ News brutes: {raw_file}")
            logger.info(f"  ‚Ä¢ News mapp√©es: {mapped_file}")

            # Sauvegarder aussi une version horodat√©e
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = Path(f"data/raw/news/hybrid_news_mapped_{timestamp}.csv")
            mapped_news.to_csv(backup_file, index=False)
            logger.info(f"  ‚Ä¢ Backup horodat√©: {backup_file}")

            logger.info(f"\n‚úÖ Toutes les donn√©es ont √©t√© sauvegard√©es avec succ√®s!")

        else:
            logger.warning("\n‚ö†Ô∏è  AUCUNE NEWS COLLECT√âE")
            logger.info("\nüí° SUGGESTIONS:")
            logger.info("  ‚Ä¢ V√©rifiez votre connexion internet")
            logger.info("  ‚Ä¢ V√©rifiez que l'API GDELT est accessible")
            logger.info("  ‚Ä¢ Essayez d'√©largir la p√©riode de collecte")
            logger.info("  ‚Ä¢ R√©duisez le min_relevance_score si n√©cessaire")

    except Exception as e:
        logger.error(f"\n‚ùå ERREUR LORS DE LA COLLECTE: {e}")
        logger.info("\nüí° V√©rifiez les logs ci-dessus pour plus de d√©tails")
        raise


if __name__ == "__main__":
    main()
