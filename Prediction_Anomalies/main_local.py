#!/usr/bin/env python3
"""
Pipeline de d√©tection d'anomalies avec donn√©es locales.
Utilise les CSV de PFE_MVP/data/raw au lieu de t√©l√©charger via yfinance.

Usage:
    # Pipeline complet avec donn√©es locales
    python main_local.py --full --period 3y

    # Actifs sp√©cifiques
    python main_local.py --full --assets APPLE TESLA "SP 500"

    # Avec filtres
    python main_local.py --full --only-critical --min-variation -15

    # √âtapes individuelles
    python main_local.py --step historical --period 1y
    python main_local.py --step detect
    python main_local.py --step correlate --max-anomalies 10
"""

import argparse
import sys
from pathlib import Path
from datetime import datetime
import os
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Ajouter le chemin du module
sys.path.insert(0, str(Path(__file__).parent))

from src.collectors.local_data_collector import LocalDataCollector
from src.detectors.anomaly_detector import AnomalyDetector
from src.correlators.newsapi_correlator import NewsAPICorrelator
from src.reporters.anomaly_report_generator import AnomalyReportGenerator
import pandas as pd


class AnomalyDetectionPipelineLocal:
    """Pipeline de d√©tection d'anomalies avec donn√©es locales."""

    def __init__(
        self,
        period: str = "3y",
        threshold_1day: float = -3.0,
        threshold_5day: float = -5.0,
        threshold_30day: float = -10.0,
        window_before: int = 2,
        window_after: int = 1,
        min_relevance: float = 20.0,
        newsapi_key: str = None,
        input_dir: str = None
    ):
        """
        Initialise le pipeline avec donn√©es locales.

        Args:
            period: P√©riode de donn√©es (1y, 3y, 5y, 10y, max)
            threshold_*: Seuils de d√©tection d'anomalies
            window_before/after: Fen√™tre de recherche news
            min_relevance: Score minimum NewsAPI
            newsapi_key: Cl√© API NewsAPI
            input_dir: R√©pertoire source des CSV (d√©faut: PFE_MVP/data/raw)
        """
        self.period = period
        self.threshold_1day = threshold_1day
        self.threshold_5day = threshold_5day
        self.threshold_30day = threshold_30day
        self.window_before = window_before
        self.window_after = window_after
        self.min_relevance = min_relevance

        # Utiliser le collector local au lieu de yfinance
        self.collector = LocalDataCollector(input_dir=input_dir)

        self.detector = AnomalyDetector(
            threshold_1day=threshold_1day,
            threshold_5day=threshold_5day,
            threshold_30day=threshold_30day
        )

        self.correlator = NewsAPICorrelator(
            api_key=newsapi_key,
            window_before=window_before,
            window_after=window_after,
            min_relevance_score=min_relevance
        )

        self.historical_data = None
        self.anomalies = None
        self.correlations = None

    def step_1_collect_historical_data(self, assets: list = None):
        """√âtape 1: Collecte des donn√©es historiques depuis les CSV locaux."""
        print("\n" + "="*70)
        print("√âTAPE 1/3: CHARGEMENT DES DONN√âES LOCALES")
        print("="*70)

        self.historical_data = self.collector.collect_and_save(
            period=self.period,
            symbols_to_fetch=assets
        )

        return bool(self.historical_data)

    def step_2_detect_anomalies(self):
        """√âtape 2: D√©tection des anomalies."""
        print("\n" + "="*70)
        print("√âTAPE 2/3: D√âTECTION DES ANOMALIES")
        print("="*70)

        if self.historical_data is None:
            print("‚ö†Ô∏è  Chargement des donn√©es historiques...")
            self.historical_data = {}

            # Charger depuis les fichiers sauvegard√©s
            historical_dir = Path("data/historical")
            if not historical_dir.exists():
                print("‚ùå Aucune donn√©e historique trouv√©e. Ex√©cutez d'abord l'√©tape 1.")
                return False

            for csv_file in historical_dir.glob("*_historical.csv"):
                asset_name = csv_file.stem.replace("_historical", "")
                self.historical_data[asset_name] = pd.read_csv(csv_file)

        if not self.historical_data:
            print("‚ùå Aucune donn√©e √† analyser")
            return False

        self.anomalies = self.detector.detect_all_anomalies(self.historical_data)

        if self.anomalies is None or len(self.anomalies) == 0:
            print("‚úÖ Aucune anomalie d√©tect√©e")
            return False

        # Afficher statistiques
        print(f"\nüìä Statistiques des anomalies d√©tect√©es:")
        severity_counts = self.anomalies['severity'].value_counts()
        for severity, count in severity_counts.items():
            print(f"  - {severity}: {count}")

        print(f"\n‚úÖ Total: {len(self.anomalies)} anomalies d√©tect√©es")
        return True

    def step_3_correlate_with_news(
        self,
        max_anomalies: int = None,
        only_critical: bool = False,
        min_variation: float = None
    ):
        """√âtape 3: Corr√©lation avec les actualit√©s."""
        print("\n" + "="*70)
        print("√âTAPE 3/3: CORR√âLATION AVEC LES ACTUALIT√âS")
        print("="*70)

        if self.anomalies is None:
            print("‚ö†Ô∏è  Chargement des anomalies...")
            anomalies_file = Path("data/anomalies/anomalies_detected.csv")
            if not anomalies_file.exists():
                print("‚ùå Aucune anomalie d√©tect√©e. Ex√©cutez d'abord l'√©tape 2.")
                return False
            self.anomalies = pd.read_csv(anomalies_file)

        # Filtrer les anomalies selon les crit√®res
        filtered_anomalies = self.anomalies.copy()

        if only_critical:
            filtered_anomalies = filtered_anomalies[
                filtered_anomalies['severity'] == 'Critical'
            ]
            print(f"üéØ Filtrage: anomalies critiques uniquement")

        if min_variation is not None:
            filtered_anomalies = filtered_anomalies[
                filtered_anomalies['variation_pct'] <= min_variation
            ]
            print(f"üéØ Filtrage: variation <= {min_variation}%")

        if len(filtered_anomalies) == 0:
            print("‚ö†Ô∏è  Aucune anomalie ne correspond aux filtres")
            return False

        print(f"üìä {len(filtered_anomalies)} anomalies apr√®s filtrage")

        # Limiter le nombre d'anomalies (pour tests NewsAPI)
        if max_anomalies and len(filtered_anomalies) > max_anomalies:
            print(f"‚ö†Ô∏è  Limitation √† {max_anomalies} anomalies (pour √©conomiser l'API)")
            filtered_anomalies = filtered_anomalies.head(max_anomalies)

        # Corr√©lation avec NewsAPI
        self.correlations = self.correlator.correlate_anomalies_with_news(
            filtered_anomalies,
            max_anomalies=max_anomalies
        )

        if self.correlations is None or len(self.correlations) == 0:
            print("‚ö†Ô∏è  Aucune corr√©lation trouv√©e")
            return False

        # G√©n√©rer les rapports
        print("\nüìù G√©n√©ration des rapports...")
        reporter = AnomalyReportGenerator()
        md_path, html_path = reporter.generate_both_reports(
            self.correlations,
            self.anomalies
        )

        print(f"\n‚úÖ Rapports g√©n√©r√©s:")
        print(f"  - Markdown: {md_path}")
        print(f"  - HTML: {html_path}")

        # Afficher les top news pour les anomalies critiques
        critical_anomalies = filtered_anomalies[
            filtered_anomalies['severity'] == 'Critical'
        ]

        if len(critical_anomalies) > 0:
            print("\n" + "="*70)
            print("TOP NEWS POUR LES ANOMALIES CRITIQUES")
            print("="*70)
            self.correlator.display_top_news_for_critical_anomalies(
                self.correlations,
                critical_anomalies
            )

        return True

    def run_full_pipeline(
        self,
        assets: list = None,
        max_anomalies: int = None,
        only_critical: bool = False,
        min_variation: float = None
    ):
        """Ex√©cute la pipeline compl√®te."""
        print("\n" + "="*70)
        print("PIPELINE DE D√âTECTION D'ANOMALIES (DONN√âES LOCALES)")
        print("="*70)
        print(f"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"üìä P√©riode: {self.period}")
        print(f"üéØ Seuils: 1j={self.threshold_1day}%, 5j={self.threshold_5day}%, 30j={self.threshold_30day}%")
        print(f"üì∞ Fen√™tre news: -{self.window_before}j √† +{self.window_after}j")
        print(f"‚≠ê Score minimum: {self.min_relevance}")

        if assets:
            print(f"üìå Actifs: {', '.join(assets)}")

        # √âtape 1: Collecte
        if not self.step_1_collect_historical_data(assets):
            print("\n‚ùå √âchec de la collecte des donn√©es")
            return False

        # √âtape 2: D√©tection
        if not self.step_2_detect_anomalies():
            print("\n‚ö†Ô∏è  Pipeline arr√™t√©e (pas d'anomalies)")
            return True

        # √âtape 3: Corr√©lation
        success = self.step_3_correlate_with_news(
            max_anomalies=max_anomalies,
            only_critical=only_critical,
            min_variation=min_variation
        )

        print("\n" + "="*70)
        if success:
            print("‚úÖ PIPELINE TERMIN√âE AVEC SUCC√àS")
        else:
            print("‚ö†Ô∏è  PIPELINE TERMIN√âE (sans corr√©lations)")
        print("="*70)

        # G√©n√©rer le fichier JSON pour le dashboard
        print("\nüìä G√©n√©ration du fichier JSON pour le dashboard...")
        try:
            import subprocess
            result = subprocess.run(
                ["python", "generate_anomalies_data.py"],
                cwd=Path(__file__).parent,
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                print("‚úÖ Fichier JSON g√©n√©r√© pour le dashboard")
            else:
                print(f"‚ö†Ô∏è  Erreur g√©n√©ration JSON: {result.stderr}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur g√©n√©ration JSON: {e}")

        return success


def main():
    """Point d'entr√©e principal."""
    parser = argparse.ArgumentParser(
        description="Pipeline de d√©tection d'anomalies avec donn√©es locales",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  # Pipeline complet
  python main_local.py --full --period 3y

  # Avec filtres
  python main_local.py --full --only-critical --max-anomalies 10

  # Actifs sp√©cifiques
  python main_local.py --full --assets APPLE TESLA

  # √âtapes individuelles
  python main_local.py --step historical --period 1y
  python main_local.py --step detect
  python main_local.py --step correlate
        """
    )

    # Mode d'ex√©cution
    parser.add_argument(
        '--full',
        action='store_true',
        help='Ex√©cuter la pipeline compl√®te'
    )
    parser.add_argument(
        '--step',
        choices=['historical', 'detect', 'correlate'],
        help='Ex√©cuter une √©tape sp√©cifique'
    )

    # Param√®tres de collecte
    parser.add_argument(
        '--period',
        type=str,
        default='3y',
        choices=['1y', '3y', '5y', '10y', 'max'],
        help='P√©riode de donn√©es (d√©faut: 3y)'
    )
    parser.add_argument(
        '--assets',
        nargs='+',
        help='Liste d\'actifs sp√©cifiques √† analyser'
    )
    parser.add_argument(
        '--input-dir',
        type=str,
        help='R√©pertoire source des CSV (d√©faut: PFE_MVP/data/raw)'
    )

    # Param√®tres de d√©tection
    parser.add_argument(
        '--threshold-1d',
        type=float,
        default=-3.0,
        help='Seuil variation 1 jour en %% (d√©faut: -3.0)'
    )
    parser.add_argument(
        '--threshold-5d',
        type=float,
        default=-5.0,
        help='Seuil variation 5 jours en %% (d√©faut: -5.0)'
    )
    parser.add_argument(
        '--threshold-30d',
        type=float,
        default=-10.0,
        help='Seuil variation 30 jours en %% (d√©faut: -10.0)'
    )

    # Param√®tres de corr√©lation
    parser.add_argument(
        '--window-before',
        type=int,
        default=2,
        help='Jours avant anomalie pour chercher news (d√©faut: 2)'
    )
    parser.add_argument(
        '--window-after',
        type=int,
        default=1,
        help='Jours apr√®s anomalie pour chercher news (d√©faut: 1)'
    )
    parser.add_argument(
        '--min-relevance',
        type=float,
        default=20.0,
        help='Score minimum de pertinence (0-100, d√©faut: 20.0)'
    )
    parser.add_argument(
        '--max-anomalies',
        type=int,
        help='Nombre max d\'anomalies √† corr√©ler (pour tests)'
    )

    # Filtres
    parser.add_argument(
        '--only-critical',
        action='store_true',
        help='Analyser uniquement les anomalies critiques'
    )
    parser.add_argument(
        '--min-variation',
        type=float,
        help='Variation minimale en %% (ex: -15 pour >= -15%%)'
    )

    args = parser.parse_args()

    # V√©rifier les arguments
    if not args.full and not args.step:
        parser.print_help()
        sys.exit(1)

    # R√©cup√©rer la cl√© NewsAPI
    newsapi_key = os.getenv('NEWSAPI_KEY')
    if not newsapi_key:
        print("‚ö†Ô∏è  NEWSAPI_KEY non trouv√©e dans .env")
        print("    La corr√©lation avec les actualit√©s sera d√©sactiv√©e")

    # Cr√©er la pipeline
    pipeline = AnomalyDetectionPipelineLocal(
        period=args.period,
        threshold_1day=args.threshold_1d,
        threshold_5day=args.threshold_5d,
        threshold_30day=args.threshold_30d,
        window_before=args.window_before,
        window_after=args.window_after,
        min_relevance=args.min_relevance,
        newsapi_key=newsapi_key,
        input_dir=args.input_dir
    )

    # Ex√©cuter selon le mode
    if args.full:
        pipeline.run_full_pipeline(
            assets=args.assets,
            max_anomalies=args.max_anomalies,
            only_critical=args.only_critical,
            min_variation=args.min_variation
        )
    elif args.step == 'historical':
        pipeline.step_1_collect_historical_data(args.assets)
    elif args.step == 'detect':
        pipeline.step_2_detect_anomalies()
    elif args.step == 'correlate':
        pipeline.step_3_correlate_with_news(
            max_anomalies=args.max_anomalies,
            only_critical=args.only_critical,
            min_variation=args.min_variation
        )


if __name__ == "__main__":
    main()
