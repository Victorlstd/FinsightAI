"""
Collecteur de news hybride: Ã©vÃ©nements macro + sectoriels
Collecte les news qui peuvent impacter les actifs sans les mentionner directement
"""
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Optional, Dict
import requests
from loguru import logger
import time
from .news_impact_mapper import NewsImpactMapper


class HybridNewsCollector:
    """
    Collecteur de news basÃ© sur l'approche hybride:
    1. Ã‰vÃ©nements macro-Ã©conomiques et gÃ©opolitiques
    2. Ã‰vÃ©nements sectoriels ciblÃ©s
    3. Mapping intelligent vers les actifs impactÃ©s
    """

    def __init__(
        self,
        output_dir: str = "data/raw/news",
        config_path: str = "config/news_strategy.yaml"
    ):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.base_url = "https://api.gdeltproject.org/api/v2/doc/doc"
        self.mapper = NewsImpactMapper(config_path)

        # Statistiques
        self.stats = {
            'total_fetched': 0,
            'macro_events': 0,
            'sector_events': 0,
            'duplicates_removed': 0
        }

    def fetch_macro_news(
        self,
        start_date: str,
        end_date: str,
        max_records_per_query: int = 250,
        delay: float = 2.0
    ) -> pd.DataFrame:
        """
        Collecte les news sur les Ã©vÃ©nements macro-Ã©conomiques

        Args:
            start_date: Date de dÃ©but (YYYY-MM-DD)
            end_date: Date de fin
            max_records_per_query: Nombre max d'articles par requÃªte
            delay: DÃ©lai entre requÃªtes (secondes)

        Returns:
            DataFrame avec les news macro
        """
        logger.info("\n" + "="*80)
        logger.info("ðŸ“° COLLECTE DES Ã‰VÃ‰NEMENTS MACRO")
        logger.info("="*80)

        all_news = []
        macro_queries = self.mapper.get_macro_event_queries()

        logger.info(f"Nombre de requÃªtes macro: {len(macro_queries)}")

        for i, query_info in enumerate(macro_queries, 1):
            logger.info(f"[{i}/{len(macro_queries)}] {query_info['event_type']}")

            df = self._fetch_gdelt_news(
                query=query_info['query'],
                start_date=start_date,
                end_date=end_date,
                max_records=max_records_per_query
            )

            if not df.empty:
                # Enrichir avec les mÃ©tadonnÃ©es de l'Ã©vÃ©nement
                df['event_type'] = query_info['event_type']
                df['event_category'] = 'macro'
                df['base_impact_score'] = query_info['impact_score']
                all_news.append(df)
                self.stats['macro_events'] += len(df)
                logger.success(f"  âœ“ {len(df)} articles collectÃ©s")

            time.sleep(delay)

        if all_news:
            combined = pd.concat(all_news, ignore_index=True)
            logger.success(f"\nâœ“ Total Ã©vÃ©nements macro: {len(combined)} articles")
            return combined
        else:
            logger.warning("Aucune news macro collectÃ©e")
            return pd.DataFrame()

    def fetch_sector_news(
        self,
        start_date: str,
        end_date: str,
        max_records_per_query: int = 250,
        delay: float = 2.0
    ) -> pd.DataFrame:
        """
        Collecte les news sur les Ã©vÃ©nements sectoriels

        Args:
            start_date: Date de dÃ©but (YYYY-MM-DD)
            end_date: Date de fin
            max_records_per_query: Nombre max d'articles par requÃªte
            delay: DÃ©lai entre requÃªtes (secondes)

        Returns:
            DataFrame avec les news sectorielles
        """
        logger.info("\n" + "="*80)
        logger.info("ðŸ­ COLLECTE DES Ã‰VÃ‰NEMENTS SECTORIELS")
        logger.info("="*80)

        all_news = []
        sector_queries = self.mapper.get_sector_event_queries()

        logger.info(f"Nombre de requÃªtes sectorielles: {len(sector_queries)}")

        for i, query_info in enumerate(sector_queries, 1):
            logger.info(f"[{i}/{len(sector_queries)}] {query_info['event_type']}")

            df = self._fetch_gdelt_news(
                query=query_info['query'],
                start_date=start_date,
                end_date=end_date,
                max_records=max_records_per_query
            )

            if not df.empty:
                # Enrichir avec les mÃ©tadonnÃ©es de l'Ã©vÃ©nement
                df['event_type'] = query_info['event_type']
                df['event_category'] = 'sector'
                df['base_impact_score'] = query_info['impact_score']
                df['affects'] = str(query_info['affects'])  # Liste des actifs affectÃ©s
                all_news.append(df)
                self.stats['sector_events'] += len(df)
                logger.success(f"  âœ“ {len(df)} articles collectÃ©s")

            time.sleep(delay)

        if all_news:
            combined = pd.concat(all_news, ignore_index=True)
            logger.success(f"\nâœ“ Total Ã©vÃ©nements sectoriels: {len(combined)} articles")
            return combined
        else:
            logger.warning("Aucune news sectorielle collectÃ©e")
            return pd.DataFrame()

    def _fetch_gdelt_news(
        self,
        query: str,
        start_date: str,
        end_date: str,
        max_records: int = 250
    ) -> pd.DataFrame:
        """
        RequÃªte GDELT pour une recherche donnÃ©e

        Args:
            query: RequÃªte de recherche
            start_date: Date de dÃ©but
            end_date: Date de fin
            max_records: Nombre max d'articles

        Returns:
            DataFrame avec les articles
        """
        try:
            # Formater les dates pour GDELT
            start_dt = self._format_date(start_date)
            end_dt = self._format_date(end_date)

            params = {
                'query': query,
                'mode': 'artlist',
                'maxrecords': min(max_records, 250),
                'format': 'json',
                'startdatetime': start_dt,
                'enddatetime': end_dt,
                'sourcelang': 'eng'  # Anglais principalement
            }

            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()

            data = response.json()

            if 'articles' not in data or not data['articles']:
                return pd.DataFrame()

            articles = data['articles']
            df = pd.DataFrame(articles)

            # Nettoyer les donnÃ©es
            if not df.empty:
                df = self._process_gdelt_data(df)
                self.stats['total_fetched'] += len(df)

            return df

        except Exception as e:
            logger.error(f"Erreur GDELT: {str(e)}")
            return pd.DataFrame()

    def _format_date(self, date_str: str) -> str:
        """Convertit une date en format GDELT (YYYYMMDDHHMMSS)"""
        if len(date_str) == 14:
            return date_str

        dt = datetime.strptime(date_str, "%Y-%m-%d")
        return dt.strftime("%Y%m%d%H%M%S")

    def _process_gdelt_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Traite et nettoie les donnÃ©es GDELT"""
        # Convertir la date
        if 'seendate' in df.columns:
            df['date'] = pd.to_datetime(df['seendate'], format='%Y%m%dT%H%M%SZ', errors='coerce')

        # Renommer les colonnes
        column_mapping = {
            'url': 'url',
            'title': 'title',
            'domain': 'source',
            'language': 'language',
            'seendate': 'published_date'
        }

        for old_col, new_col in column_mapping.items():
            if old_col in df.columns and old_col != new_col:
                df = df.rename(columns={old_col: new_col})

        # SÃ©lectionner les colonnes importantes
        important_cols = ['date', 'title', 'url', 'source', 'language']
        available_cols = [col for col in important_cols if col in df.columns]

        # Garder aussi les colonnes qu'on a ajoutÃ©es
        extra_cols = ['event_type', 'event_category', 'base_impact_score', 'affects']
        for col in extra_cols:
            if col in df.columns:
                available_cols.append(col)

        df = df[available_cols]

        return df

    def fetch_all_news(
        self,
        start_date: str,
        end_date: str,
        max_records_per_query: int = 250,
        delay: float = 2.0
    ) -> pd.DataFrame:
        """
        Collecte toutes les news (macro + sectorielles)

        Args:
            start_date: Date de dÃ©but (YYYY-MM-DD)
            end_date: Date de fin
            max_records_per_query: Nombre max d'articles par requÃªte
            delay: DÃ©lai entre requÃªtes

        Returns:
            DataFrame combinÃ© avec toutes les news
        """
        logger.info("\n" + "="*80)
        logger.info("ðŸš€ DÃ‰MARRAGE COLLECTE HYBRIDE DE NEWS")
        logger.info("="*80)
        logger.info(f"PÃ©riode: {start_date} â†’ {end_date}")

        # Reset stats
        self.stats = {
            'total_fetched': 0,
            'macro_events': 0,
            'sector_events': 0,
            'duplicates_removed': 0
        }

        # Collecte macro
        macro_df = self.fetch_macro_news(start_date, end_date, max_records_per_query, delay)

        # Collecte sectorielle
        sector_df = self.fetch_sector_news(start_date, end_date, max_records_per_query, delay)

        # Combiner
        all_news = []
        if not macro_df.empty:
            all_news.append(macro_df)
        if not sector_df.empty:
            all_news.append(sector_df)

        if not all_news:
            logger.warning("Aucune news collectÃ©e")
            return pd.DataFrame()

        combined = pd.concat(all_news, ignore_index=True)

        # DÃ©dupliquer par URL
        initial_count = len(combined)
        combined = combined.drop_duplicates(subset=['url'], keep='first')
        self.stats['duplicates_removed'] = initial_count - len(combined)

        logger.info("\n" + "="*80)
        logger.info("ðŸ“Š STATISTIQUES DE COLLECTE")
        logger.info("="*80)
        logger.info(f"Total articles collectÃ©s: {self.stats['total_fetched']}")
        logger.info(f"  - Ã‰vÃ©nements macro: {self.stats['macro_events']}")
        logger.info(f"  - Ã‰vÃ©nements sectoriels: {self.stats['sector_events']}")
        logger.info(f"Doublons supprimÃ©s: {self.stats['duplicates_removed']}")
        logger.info(f"Articles uniques finaux: {len(combined)}")
        logger.info("="*80)

        return combined

    def map_news_to_assets(
        self,
        news_df: pd.DataFrame,
        min_relevance_score: float = 5.0,
        top_n_guaranteed: int = 0
    ) -> pd.DataFrame:
        """
        Mappe chaque news aux actifs qu'elle peut impacter

        Args:
            news_df: DataFrame avec les news collectÃ©es
            min_relevance_score: Score minimum de pertinence
            top_n_guaranteed: Nombre de news top Ã  garantir (mÃªme si score < min)
                            0 = dÃ©sactivÃ©, 50/100 = garantit les N meilleures news

        Returns:
            DataFrame enrichi avec les actifs impactÃ©s et leurs scores
        """
        logger.info("\n" + "="*80)
        logger.info("ðŸ”— MAPPING DES NEWS VERS LES ACTIFS")
        logger.info("="*80)

        if top_n_guaranteed > 0:
            logger.info(f"ðŸŽ¯ Mode garantie activÃ©: Top {top_n_guaranteed} news les plus importantes")

        if news_df.empty:
            return pd.DataFrame()

        # PremiÃ¨re passe: calculer les scores pour toutes les news
        all_scores = []

        for idx, row in news_df.iterrows():
            title = row.get('title', '')

            # Utiliser le mapper avec score minimum = 0 pour tout Ã©valuer
            relevant_assets = self.mapper.get_relevant_assets_for_news(
                news_title=title,
                news_content="",
                min_score=0.0  # Pas de filtrage ici
            )

            if relevant_assets:
                for asset_name, score, matched_events in relevant_assets:
                    all_scores.append({
                        'row': row,
                        'asset': asset_name,
                        'score': score,
                        'matched_events': matched_events
                    })

        if not all_scores:
            logger.warning("Aucune news n'a pu Ãªtre Ã©valuÃ©e")
            return pd.DataFrame()

        # CrÃ©er un DataFrame temporaire pour l'analyse
        temp_df = pd.DataFrame(all_scores)

        # DÃ©terminer le seuil effectif
        if top_n_guaranteed > 0:
            # Trier par score dÃ©croissant
            temp_df_sorted = temp_df.sort_values('score', ascending=False)

            # Prendre les top N scores
            if len(temp_df_sorted) > top_n_guaranteed:
                nth_score = temp_df_sorted.iloc[top_n_guaranteed - 1]['score']
                effective_threshold = min(min_relevance_score, nth_score)
                logger.info(f"Seuil effectif ajustÃ©: {effective_threshold:.2f} (garantit top {top_n_guaranteed})")
            else:
                effective_threshold = min_relevance_score
                logger.info(f"Moins de {top_n_guaranteed} news, seuil standard appliquÃ©: {effective_threshold:.2f}")
        else:
            effective_threshold = min_relevance_score
            logger.info(f"Seuil de pertinence: {effective_threshold:.2f}")

        # Filtrer selon le seuil effectif
        results = []
        for item in all_scores:
            if item['score'] >= effective_threshold:
                result_row = item['row'].copy()
                result_row['asset'] = item['asset']
                result_row['relevance_score'] = item['score']
                result_row['matched_events'] = ', '.join(item['matched_events'])
                results.append(result_row)

        if not results:
            logger.warning("Aucune news n'atteint le score minimum de pertinence")
            return pd.DataFrame()

        mapped_df = pd.DataFrame(results)

        logger.success(f"âœ“ {len(mapped_df)} associations news-actifs crÃ©Ã©es")
        logger.info(f"Nombre de news uniques: {mapped_df['url'].nunique()}")
        logger.info(f"Nombre d'actifs impactÃ©s: {mapped_df['asset'].nunique()}")

        # Statistiques sur les scores
        if top_n_guaranteed > 0:
            below_min = len(mapped_df[mapped_df['relevance_score'] < min_relevance_score])
            if below_min > 0:
                logger.info(f"ðŸ“Š {below_min} associations capturÃ©es grÃ¢ce au mode garantie (score < {min_relevance_score})")

        # Afficher les top actifs impactÃ©s
        top_assets = mapped_df['asset'].value_counts().head(10)
        logger.info("\nðŸ“Š Top 10 actifs les plus mentionnÃ©s:")
        for asset, count in top_assets.items():
            logger.info(f"  {asset}: {count} news")

        return mapped_df

    def collect_and_map(
        self,
        start_date: str,
        end_date: str,
        min_relevance_score: float = 5.0,
        max_records_per_query: int = 250,
        delay: float = 2.0,
        top_n_guaranteed: int = 0
    ) -> pd.DataFrame:
        """
        Pipeline complÃ¨te: collecte + mapping

        Args:
            start_date: Date de dÃ©but
            end_date: Date de fin
            min_relevance_score: Score minimum de pertinence
            max_records_per_query: Nombre max d'articles par requÃªte
            delay: DÃ©lai entre requÃªtes
            top_n_guaranteed: Garantir les N news les plus importantes (0 = dÃ©sactivÃ©)

        Returns:
            DataFrame avec news mappÃ©es aux actifs
        """
        # Collecte
        news_df = self.fetch_all_news(start_date, end_date, max_records_per_query, delay)

        if news_df.empty:
            return pd.DataFrame()

        # Sauvegarder les news brutes
        raw_path = self.output_dir / "hybrid_news_raw.csv"
        news_df.to_csv(raw_path, index=False)
        logger.info(f"\nðŸ’¾ News brutes sauvegardÃ©es: {raw_path}")

        # Mapping
        mapped_df = self.map_news_to_assets(news_df, min_relevance_score, top_n_guaranteed)

        if not mapped_df.empty:
            # Sauvegarder les news mappÃ©es
            mapped_path = self.output_dir / "hybrid_news_mapped.csv"
            mapped_df.to_csv(mapped_path, index=False)
            logger.success(f"ðŸ’¾ News mappÃ©es sauvegardÃ©es: {mapped_path}")

        return mapped_df


if __name__ == "__main__":
    # Test du collecteur hybride
    from src.utils.logger import setup_logger

    setup_logger()

    collector = HybridNewsCollector()

    # Test sur une courte pÃ©riode
    logger.info("ðŸ§ª TEST DU COLLECTEUR HYBRIDE")

    mapped_news = collector.collect_and_map(
        start_date="2024-01-15",
        end_date="2024-01-20",
        min_relevance_score=5.0,
        max_records_per_query=50,  # RÃ©duit pour le test
        delay=2.0
    )

    if not mapped_news.empty:
        print("\n" + "="*80)
        print("APERÃ‡U DES RÃ‰SULTATS")
        print("="*80)
        print(f"\nNombre total d'associations: {len(mapped_news)}")
        print(f"\nColonnes: {mapped_news.columns.tolist()}")
        print(f"\nPremiers rÃ©sultats:")
        print(mapped_news[['title', 'asset', 'relevance_score', 'event_type']].head(10))
