# ğŸ“° ModÃ¨le d'Analyse de Sentiment pour News FinanciÃ¨res

> ModÃ¨le FinBERT fine-tunÃ© pour l'analyse de sentiment de news financiÃ¨res longues (articles, communiquÃ©s de presse, etc.)

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [Architecture du modÃ¨le](#architecture-du-modÃ¨le)
- [Installation](#installation)
- [Utilisation rapide](#utilisation-rapide)
- [IntÃ©gration dans un pipeline](#intÃ©gration-dans-un-pipeline)
- [API Ready Function](#api-ready-function)
- [Format des donnÃ©es](#format-des-donnÃ©es)
- [Performance](#performance)
- [Limitations](#limitations)

---

## ğŸ¯ Vue d'ensemble

Ce modÃ¨le analyse le sentiment (positif/nÃ©gatif) de textes financiers longs comme des articles de presse, des communiquÃ©s d'entreprise, etc.

### CaractÃ©ristiques principales

- **ModÃ¨le de base** : [ProsusAI/finbert](https://huggingface.co/ProsusAI/finbert)
- **TÃ¢che** : Classification binaire (Positif / NÃ©gatif)
- **Longueur maximale** : 512 tokens (~300-400 mots)
- **Format** : SafeTensors (sÃ©curisÃ© et rapide)
- **DonnÃ©es d'entraÃ®nement** : 
  - Yahoo News financiÃ¨res
  - Financial Phrase Bank
  - Corpus all-data.csv
  - **Total** : ~70,000 exemples Ã©quilibrÃ©s

### DiffÃ©rence avec le modÃ¨le Tweets

| Aspect | News Model | Tweets Model |
|--------|-----------|--------------|
| **Max Length** | 512 tokens | 128 tokens |
| **Use Case** | Articles longs | Textes courts |
| **Training Data** | News financiÃ¨res | Tweets financiers |
| **Chemin** | `./news_finbert_sentiment_model` | `./tweets_finbert_sentiment_model` |

---

## ğŸ—ï¸ Architecture du modÃ¨le

```
Input Text (max 512 tokens)
        â†“
   Tokenization (FinBERT Tokenizer)
        â†“
   BERT Encoder (12 layers)
        â†“
   Classification Head (2 classes)
        â†“
   Output: [Negative, Positive] probabilities
```

**Labels**:
- `0` â†’ Negative
- `1` â†’ Positive

---

## ğŸš€ Installation

### PrÃ©requis

```bash
pip install torch transformers pandas newsapi-python python-dotenv
```

### Structure des fichiers

```
NLP/
â”œâ”€â”€ news_finbert_sentiment_model/    # ModÃ¨le fine-tunÃ©
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ model.safetensors
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ vocab.txt
â”œâ”€â”€ News_API_Integration.ipynb       # Exemple d'intÃ©gration
â””â”€â”€ .env                             # ClÃ©s API (NEWSAPI_API_KEY)
```

---

## âš¡ Utilisation rapide

### 1. Chargement du modÃ¨le

```python
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Charger le modÃ¨le
MODEL_PATH = "./news_finbert_sentiment_model"
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_PATH,
    num_labels=2,
    use_safetensors=True
)
model.to(device)
model.eval()

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
```

### 2. Analyse d'un texte

```python
def analyze_sentiment(text, model, tokenizer, device, max_length=512):
    """
    Analyse le sentiment d'un texte financier
    
    Args:
        text (str): Texte Ã  analyser
        model: ModÃ¨le FinBERT
        tokenizer: Tokenizer FinBERT
        device: torch.device
        max_length (int): Longueur max (512 pour news)
    
    Returns:
        dict: {
            'sentiment': 'Positive' ou 'Negative',
            'confidence': float (0-1),
            'prob_negative': float,
            'prob_positive': float
        }
    """
    if not text or len(text.strip()) == 0:
        return {
            'sentiment': 'Unknown',
            'confidence': 0.0,
            'prob_negative': 0.5,
            'prob_positive': 0.5
        }
    
    # Tokenization
    encoding = tokenizer(
        text,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )
    
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)
    
    # PrÃ©diction
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        probabilities = F.softmax(logits, dim=1)[0]
        
        prediction = torch.argmax(logits, dim=1).item()
        confidence = probabilities[prediction].item()
    
    sentiment_label = "Positive" if prediction == 1 else "Negative"
    
    return {
        'sentiment': sentiment_label,
        'confidence': confidence,
        'prob_negative': probabilities[0].item(),
        'prob_positive': probabilities[1].item()
    }
```

### 3. Exemple d'utilisation

```python
# Texte d'exemple
text = """
Apple Inc. reported record-breaking quarterly earnings, exceeding analyst 
expectations. Revenue grew 28% year-over-year, driven by strong iPhone sales 
and expanding services business. The company announced a $90 billion share 
buyback program and raised its dividend by 7%.
"""

# Analyser
result = analyze_sentiment(text, model, tokenizer, device)

print(f"Sentiment: {result['sentiment']}")
print(f"Confidence: {result['confidence']:.2%}")
print(f"Prob Positive: {result['prob_positive']:.2%}")
print(f"Prob Negative: {result['prob_negative']:.2%}")
```

**Output**:
```
Sentiment: Positive
Confidence: 94.23%
Prob Positive: 94.23%
Prob Negative: 5.77%
```

---

## ğŸ”„ IntÃ©gration dans un pipeline

### Pipeline complet : Fetch + Analyze

```python
from newsapi.newsapi_client import NewsApiClient
from datetime import datetime, timedelta
import pandas as pd

# Initialiser NewsAPI
newsapi = NewsApiClient(api_key="YOUR_API_KEY")

def analyze_news_pipeline(query="finance", page_size=20):
    """
    Pipeline complet : rÃ©cupÃ¨re les news et analyse le sentiment
    
    Args:
        query (str): RequÃªte de recherche (ex: "Apple OR Tesla")
        page_size (int): Nombre d'articles Ã  rÃ©cupÃ©rer
    
    Returns:
        pd.DataFrame: RÃ©sultats avec sentiment analysis
    """
    # 1. RÃ©cupÃ©rer les news
    from_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
    
    articles = newsapi.get_everything(
        q=query,
        language='en',
        from_param=from_date,
        sort_by='publishedAt',
        page_size=page_size
    )
    
    # 2. Analyser chaque article
    results = []
    for article in articles.get('articles', []):
        # Combiner titre + description
        full_text = f"{article['title']} {article.get('description', '')}"
        
        # Analyser le sentiment
        sentiment_result = analyze_sentiment(full_text, model, tokenizer, device)
        
        # Stocker les rÃ©sultats
        results.append({
            'title': article['title'],
            'source': article['source']['name'],
            'published_at': article['publishedAt'],
            'sentiment': sentiment_result['sentiment'],
            'confidence': sentiment_result['confidence'],
            'prob_negative': sentiment_result['prob_negative'],
            'prob_positive': sentiment_result['prob_positive'],
            'url': article['url']
        })
    
    return pd.DataFrame(results)

# Utilisation
df = analyze_news_pipeline(query="stocks OR market", page_size=50)

# Filtrer par confiance Ã©levÃ©e
df_high_confidence = df[df['confidence'] > 0.8]

print(f"Articles analysÃ©s: {len(df)}")
print(f"Articles haute confiance (>80%): {len(df_high_confidence)}")
print(f"Sentiment positif: {(df['sentiment'] == 'Positive').sum()}")
print(f"Sentiment nÃ©gatif: {(df['sentiment'] == 'Negative').sum()}")
```

### Export des rÃ©sultats

```python
# CSV
output_file = f"sentiment_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
df.to_csv(output_file, index=False)

# JSON (pour API)
output_json = output_file.replace('.csv', '.json')
df.to_json(output_json, orient='records', indent=2)
```

---

## ğŸŒ API Ready Function

Fonction prÃªte pour intÃ©gration backend/frontend:

```python
def get_sentiment_analysis(query, max_results=20):
    """
    Fonction API-ready pour l'Ã©quipe backend/frontend
    
    Args:
        query (str): RequÃªte de recherche
        max_results (int): Nombre max de rÃ©sultats
    
    Returns:
        dict: Format standardisÃ© pour API
    """
    df = analyze_news_pipeline(query=query, page_size=max_results)
    
    if df.empty:
        return {
            "status": "error",
            "message": "No news found",
            "data": []
        }
    
    return {
        "status": "success",
        "query": query,
        "total_articles": len(df),
        "positive_count": int((df['sentiment'] == 'Positive').sum()),
        "negative_count": int((df['sentiment'] == 'Negative').sum()),
        "average_confidence": float(df['confidence'].mean()),
        "timestamp": datetime.now().isoformat(),
        "articles": df.to_dict('records')
    }

# Exemple
result = get_sentiment_analysis("cryptocurrency", max_results=10)
print(f"Status: {result['status']}")
print(f"Total articles: {result['total_articles']}")
print(f"Average confidence: {result['average_confidence']:.2%}")
```

**Exemple de rÃ©ponse JSON**:
```json
{
  "status": "success",
  "query": "cryptocurrency",
  "total_articles": 10,
  "positive_count": 6,
  "negative_count": 4,
  "average_confidence": 0.87,
  "timestamp": "2026-01-20T14:30:00",
  "articles": [
    {
      "title": "Bitcoin reaches new all-time high",
      "source": "CoinDesk",
      "published_at": "2026-01-20T10:00:00Z",
      "sentiment": "Positive",
      "confidence": 0.94,
      "prob_positive": 0.94,
      "prob_negative": 0.06,
      "url": "https://..."
    }
  ]
}
```

---

## ğŸ“Š Format des donnÃ©es

### Input

```python
{
    "text": str,           # Texte Ã  analyser (max 512 tokens)
    "max_length": int     # Optional, dÃ©faut: 512
}
```

### Output

```python
{
    "sentiment": str,        # "Positive" ou "Negative"
    "confidence": float,     # 0.0 Ã  1.0
    "prob_negative": float,  # ProbabilitÃ© classe nÃ©gative
    "prob_positive": float   # ProbabilitÃ© classe positive
}
```

### Traitement par batch

```python
def analyze_batch(texts, model, tokenizer, device, batch_size=8):
    """Analyse un batch de textes pour meilleure performance"""
    results = []
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        
        # Tokenize batch
        encodings = tokenizer(
            batch_texts,
            add_special_tokens=True,
            max_length=512,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        input_ids = encodings['input_ids'].to(device)
        attention_mask = encodings['attention_mask'].to(device)
        
        # PrÃ©dictions
        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            probabilities = F.softmax(outputs.logits, dim=1)
            predictions = torch.argmax(outputs.logits, dim=1)
        
        # Formater rÃ©sultats
        for j, pred in enumerate(predictions):
            results.append({
                'sentiment': 'Positive' if pred.item() == 1 else 'Negative',
                'confidence': probabilities[j][pred.item()].item(),
                'prob_negative': probabilities[j][0].item(),
                'prob_positive': probabilities[j][1].item()
            })
    
    return results
```

---

## ğŸ“ˆ Performance

### MÃ©triques sur le test set

```
Accuracy:  0.8923
Precision: 0.8956
Recall:    0.8891
F1-Score:  0.8923
```

### Rapport de classification

```
              precision    recall  f1-score   support

     NÃ©gatif     0.89      0.90      0.89      5234
     Positif     0.90      0.89      0.89      5287

    accuracy                         0.89     10521
   macro avg     0.89      0.89      0.89     10521
weighted avg     0.89      0.89      0.89     10521
```

### Temps d'infÃ©rence

| Device | Batch Size | Temps par article |
|--------|-----------|-------------------|
| CPU    | 1         | ~150ms           |
| CPU    | 8         | ~80ms            |
| GPU    | 1         | ~20ms            |
| GPU    | 32        | ~5ms             |

---

## âš ï¸ Limitations

### 1. Longueur de texte
- **Maximum**: 512 tokens (~300-400 mots)
- **Recommandation**: Pour articles trÃ¨s longs, analyser le titre + lead (premiers paragraphes)

```python
# Exemple de gestion de textes longs
def prepare_long_text(title, content, max_words=300):
    """PrÃ©pare un article long pour l'analyse"""
    words = content.split()[:max_words]
    truncated_content = ' '.join(words)
    return f"{title} {truncated_content}"
```

### 2. Langue
- **OptimisÃ© pour**: Anglais
- **Autres langues**: Performances rÃ©duites (modÃ¨le entraÃ®nÃ© sur corpus anglais)

### 3. Domaine
- **OptimisÃ© pour**: Finance, Ã©conomie, business
- **Hors domaine**: Peut Ãªtre moins performant sur textes gÃ©nÃ©raux

### 4. NeutralitÃ©
- Le modÃ¨le est entraÃ®nÃ© sur **binaire** (Positif/NÃ©gatif)
- Pas de classe "Neutre" â†’ textes neutres sont classÃ©s comme Positif ou NÃ©gatif
- **Solution**: Filtrer par `confidence` pour ignorer prÃ©dictions peu sÃ»res

```python
# Filtrer prÃ©dictions incertaines
def filter_confident_predictions(df, threshold=0.75):
    """Garde seulement les prÃ©dictions avec confiance > threshold"""
    return df[df['confidence'] > threshold]
```

---

## ğŸ”§ Configuration avancÃ©e

### Ajuster la confiance minimum

```python
def analyze_with_threshold(text, model, tokenizer, device, threshold=0.8):
    """Retourne sentiment seulement si confiance > threshold"""
    result = analyze_sentiment(text, model, tokenizer, device)
    
    if result['confidence'] < threshold:
        result['sentiment'] = 'Uncertain'
    
    return result
```

### Logging dÃ©taillÃ©

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def analyze_with_logging(text, model, tokenizer, device):
    """Version avec logging pour debugging"""
    logger.info(f"Analyzing text (length: {len(text)} chars)")
    
    result = analyze_sentiment(text, model, tokenizer, device)
    
    logger.info(f"Prediction: {result['sentiment']} (conf: {result['confidence']:.2%})")
    
    return result
```

---

## ğŸ“š Ressources

- **Notebook d'entraÃ®nement**: [Analyse_News.ipynb](./Analyse_News.ipynb)
- **IntÃ©gration API**: [News_API_Integration.ipynb](./News_API_Integration.ipynb)
- **ModÃ¨le Hugging Face**: [ProsusAI/finbert](https://huggingface.co/ProsusAI/finbert)
- **NewsAPI Documentation**: [newsapi.org/docs](https://newsapi.org/docs)

---

## ğŸ¤ Support

Pour toute question ou problÃ¨me:

1. VÃ©rifier les exemples dans `News_API_Integration.ipynb`
2. Consulter la section [Limitations](#limitations)
3. VÃ©rifier les versions des dÃ©pendances

### DÃ©pendances recommandÃ©es

```txt
torch>=2.0.0
transformers>=4.30.0
pandas>=1.5.0
newsapi-python>=0.2.7
python-dotenv>=1.0.0
```

---

## ğŸ“ Changelog

### v1.0.0 (2026-01-20)
- âœ… ModÃ¨le FinBERT fine-tunÃ© sur 70k+ exemples
- âœ… Support sequences longues (512 tokens)
- âœ… Format SafeTensors
- âœ… Pipeline NewsAPI intÃ©grÃ©
- âœ… API-ready functions

---

**DerniÃ¨re mise Ã  jour**: 20 janvier 2026  
**Version du modÃ¨le**: 1.0.0  
**Auteur**: Ã‰quipe NLP - FinsightAI
